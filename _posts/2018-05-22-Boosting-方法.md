---
layout:     post
title:      Boosting 方法
subtitle:   从决策树到提升方法，到GBDT和xgboost
date:       2018-05-22
author:     Xiya Lv
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - Machine Learning
    - 决策树
    - boosting
    - GBDT
---

# 前言

​	本文主要分析adaboost、GBDT和xgboost等提升方法。本文从决策树开始，梳理出boosting算法的原理，和各种不同boosting算法之间的差异和适用场景，并结合具体案例进行分析。

​	本文主要内容如下：

  - 决策树
      - 原理
      - 算法
      - CART算法
- Boosting
  - 原理
  - adaboost
  - GBDT
  - xgboost

# 决策树

### 原理

​	决策树是一种基本的分类和回归方法，通过树形结构对实例进行分类（回归同理），可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。决策树本质上是从训练数据集中归纳出一组分类规则（对应if-then规则），从另一个角度，决策树学习是由训练集估计条件概率模型。

​	决策树可能有多个，目的是选出一个最优的决策树，但是这个问题是一个NP问题，因此，决策树算法是一种启发式方法，选择的结果是次最优的。决策树算法通常是一个递归选择最优特征，并根据该特征对特征空间进行划分，对应着决策树的构建的一种算法。

​	优点：可读性高，分类速度快。

​	决策树主要分三个步骤：特征选择，决策树生成和决策树剪枝。

​	决策树算法主要有ID3和C4.5，用以分类问题；CART既能分类，又能做回归问题。

​	

​	接下来从决策树的三个步骤分别进行阐述：

##### 特征选择

​	特征选择的标准是**信息增益**或**信息增益比**。

- 名词解释

  - 熵（entropy）$H(X)$

    熵是随机变量不稳定的度量。（我们想要的是熵小）

    设$X$是一个取有限个离散的随机变量，概率分布为

    ​	$$P(X=x_i)=p_i, i=1,2…n$$,

    则熵定义为：

    ​	$$H(X)=-\sum\limits_{i=1}^n p_ilogp_i$$

    若$p_i=0$，则认为$0log0=0$，若log以2为底，熵的单位是比特（bit），若log以e为底，熵的单位是纳特（nat）。

    $$ 0 \leq H(X)\leq logn$$

    当熵由数据计算出来的时候，称为经验熵。

  - 条件熵（conditional entropy）$H(Y|X)$

    $H(Y|X)$表示已知随机变量X的条件下随机变量Y的不确定性。

    ​	$$H(Y|X)=\sum\limits_{i=1}^np_iH(Y|X=x_i)$$, 其中$p_i=P(X=x_i), i=1,2…n$

    当条件熵有数据计算出来的时候，称为经验条件熵。

  - 信息增益

    信息增益就是经验熵和经验条件熵之差，也叫互信息（mutual information）

    ​	$$g(D,A) = H(D)-H(D|A)$$

    可以认为这里的D是label，A为特征的一个。

  - 信息增益比

    ​	$$g_R(D,A)=\frac{g(D,A)}{H_A(D)}$$,

    其中$H_A(D)=-\sum\limits_{i=1}^n \frac{|D_i|}{|D|}log_2\frac{|D_i|}{D}$, n是特征A取值的个数。

- 信息增益算法

  ```
  
  ```

  

  

