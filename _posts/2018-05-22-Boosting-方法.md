---
layout:     post
title:      Boosting 方法
subtitle:   从决策树到提升方法，到GBDT和xgboost
date:       2018-05-22
author:     Xiya Lv
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - Machine Learning
    - 决策树
    - boosting
    - GBDT
---

# 前言

​	本文主要分析adaboost、GBDT和xgboost等提升方法。本文从决策树开始，梳理出boosting算法的原理，和各种不同boosting算法之间的差异和适用场景，并结合具体案例进行分析。

​	本文主要内容如下：

  - 决策树
      - 原理

      - CART算法
- Boosting
  - 原理
  - adaboost
  - GBDT
  - xgboost

# 决策树

### 原理

​	决策树是一种基本的分类和回归方法，通过树形结构对实例进行分类（回归同理），可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。决策树本质上是从训练数据集中归纳出一组分类规则（对应if-then规则），从另一个角度，决策树学习是由训练集估计条件概率模型。

​	决策树可能有多个，目的是选出一个最优的决策树，但是这个问题是一个NP问题，因此，决策树算法是一种启发式方法，选择的结果是次最优的。决策树算法通常是一个递归选择最优特征，并根据该特征对特征空间进行划分，对应着决策树的构建的一种算法。

​	优点：可读性高，分类速度快。

​	决策树主要分三个步骤：特征选择，决策树生成和决策树剪枝。

​	决策树算法主要有ID3和C4.5，用以分类问题；CART既能分类，又能做回归问题。

​	接下来从决策树的三个步骤分别进行阐述：

##### 特征选择

​	特征选择的标准是**信息增益**或**信息增益比**。

- 名词解释

  - 熵（entropy）$H(X)$

    熵是随机变量不稳定的度量。（我们想要的是熵小）
    
    设$X$是一个取有限个离散的随机变量，概率分布为
    
    <img src="http://latex.codecogs.com/gif.latex?P(X=x_i)=p_i, i=1,2…n" />
    
    则熵定义为：
    <img src="http://latex.codecogs.com/gif.latex?H(X)=-\sum \limits_{i=1}^n p_ilogp_i" />
    
    若$p_i=0$，则认为$0log0=0$，若log以2为底，熵的单位是比特（bit），若log以e为底，熵的单位是纳特（nat）。
    
    <img src="http://latex.codecogs.com/gif.latex?0 \leq H(X) \leq logn" />
    
    当熵由数据计算出来的时候，称为经验熵。

  - 条件熵（conditional entropy）
    <img src="http://latex.codecogs.com/gif.latex?H(Y|X)" />

    表示已知随机变量X的条件下随机变量Y的不确定性。
    <img src="http://latex.codecogs.com/gif.latex?H(Y|X)=\sum \limits_{i=1}^np_iH(Y|X=x_i)" />
    
    其中,  $$p_i=P(X=x_i), i=1,2…n$$
    
    当条件熵有数据计算出来的时候，称为经验条件熵。

  - 信息增益

    信息增益就是经验熵和经验条件熵之差，也叫互信息（mutual information）

    ​	$$g(D,A) = H(D)-H(D \mid A)$$

    可以认为这里的D是label，A为特征的一个。

  - 信息增益比

    ​	$$g_R(D,A)=\frac{g(D, A)}{H_A(D)}$$,

    其中$$H_A(D)=-\sum\limits_{i=1}^n \frac{\|D_i\|}{\|D\|}log_2\frac{\|D_i\|}{D}$$, n是特征A取值的个数。

- 信息增益算法

  1. 计算经验熵H(D)
  2. 计算经验条件熵$H(D \mid A)$
  3. 计算信息增益 $g(D \mid A) = H(D) - H(D \mid A)$

##### 决策树生成

- 主要有两个算法：ID3和C4.5
- 两者的区别在于ID3采用信息增益来选择特征；C4.5采用信息增益比来选择特征；

##### 决策树的剪枝

​	决策树的剪枝往往通过极小化决策树整体的损失函数（loss function）来实现。损失函数定义为：

​		$$C_a(T) = \sum \limits_{t=1}^{\|T\|}N_tH_t(T) + a\|T\|$$

其中，$H_t(T)$为叶节点t上的经验熵，$a \geq 0$为参数。经验熵$$H_t(T) = -\sum\limits_k\frac{N_{tk}}{N_t}log\frac{N_{tk}}{N_t}$$, 因此上式的第一项就变成了$C(T) = - \sum \limits_{t=1}^{\|T\|}\sum\limits_{k=1}^KN_{tk}log\frac{N_{tk}}{N_t}$, 此时上式就变成了 $C_a{T}=C(T)+a\|T\|$, 第一项表示模型对训练数据的预测误差，$\|T\|$表示模型复杂度，参数$a$控制两者的关系。较大的$a$促使选择较简单的模型树，较小的$a$促使选择较复杂的模型树。

### CART算法

##### 原理

​	CART算法即可用于分类，也可用于回归。是在给定输入随机变量X条件下输出随机变量Y的条件概率分布的学习方法。**CART中的决策树是二叉树。**

​	CART算法由两步组成：决策树生成，且生成的决策树要尽量大；决策树剪枝，以损失函数最小为剪枝的标准。

##### CART生成

生成CART针对分类问题和回归问题所采用的准则不同，分类问题用基尼指数（Gini index），回归问题用平方误差最小化准则。

![WechatIMG33005](../img/2018-05-22-Boosting-方法/1.jpg)

```
CART的算法：
输入：CART算法生成的决策树$T_0$
输出：最优决策树$T_a$
1. 设k=0，T=$T_0$, $a=+fin$
2. 自下而上对各内部结点t计算$C(T_t)$, $\|T_t\|$以及
	$g(t)=\frac{C(t)-C(T_t)}{\|T_t\|-1}$
	$a=min(a, g(t))$
	这里，$T_t$表示以t为根节点的子树，$C(T_t)$是对训练数据的预测误差，$\|T_t\|$是$T_t$的叶节点的个数。C(t)表示t为单个结点时的对训练数据的预测误差。
3. 对$g(t)=a$的内部结点t进行剪枝，并对叶节点t以多数表决法决定其类，得到树T。
4. 设$k=k+1, a_k=a, T_k=T$
5. 如果$T_k$不是由根节点及两个叶节点构成的树，则回到步骤3；否则令$T_k=T_n$
6. 采用交叉验证法在子树序列$T_0,T_1,...T_n$中选取最有子树$T_a$.
```

















